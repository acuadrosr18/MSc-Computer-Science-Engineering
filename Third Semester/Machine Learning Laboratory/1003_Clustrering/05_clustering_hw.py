# -*- coding: utf-8 -*-
"""05_Clustering_HW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mopcY2uyEFI1zn4gecJuPLM0320qagDm
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler , OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.metrics import silhouette_score

pd.set_option('display.max_columns', None)

"""# HW:
The data set includes the churn of customers of a telecommunications company.
The task is to create segments from customers based on their characteristics using the KMeans algorithm.

Do not use the following variables for grouping:
- churn?: has the customer dropped out?
- Contract_date: contract conclusion time
- Cust_ID: customer ID
"""

file_path = "/content/telco_sampled.csv"
df = pd.read_csv(file_path, sep = ';')

df.head()

df['churn?'].value_counts()

"""# 1. Subtask: (data preparation)
Use all variables except for the three variables above when creating the clusters. Perform data preparation so that the variables are input to the model in the appropriate form.

(hint: categorical variables, missing values, scaling, etc.)
"""

# Dropping the columns not required for clustering
df = df.drop(columns=['churn?', 'Contract_date', 'Cust_ID'])

# Handling categorical variables with OneHotEncoding and missing values
categorical_features = df.select_dtypes(include=['object']).columns.tolist()
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Display the identified features for preprocessing
print("\nCategorical Features:", categorical_features)
print("Numerical Features:", numerical_features)

# Apply scaling only to numerical columns
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Pipelines for numerical and categorical data processing
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(drop='first'))
])

# Combine pipelines using ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_pipeline, numerical_features),
    ('cat', categorical_pipeline, categorical_features)
])

# Preprocess the data
X_preprocessed = preprocessor.fit_transform(df)

# Display shape and preview of preprocessed data
print("Preprocessed Data Shape:", X_preprocessed.shape)
print(pd.DataFrame(X_preprocessed).head())

"""# 2. Subtask: (clustering)
Find the optimal k value for the KMeans algorithm using the variables prepared in the previous task. Then group the customers.
"""

# Function to apply only the Elbow Method
def elbow_method(X):
    wcss = []  # Within-cluster sum of squares (WCSS) for the elbow method
    K_values = range(1, 11)  # Trying out K values from 1 to 10

    # Loop to calculate WCSS for each K value
    for k in K_values:
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)

    # Plotting the elbow graph
    plt.figure(figsize=(8, 5))
    plt.plot(K_values, wcss, 'bo-')
    plt.title('Elbow Method for Optimal K')
    plt.xlabel('Number of clusters (K)')
    plt.ylabel('WCSS (Inertia)')
    plt.show()

# Assuming X_preprocessed is already available from previous steps
elbow_method(X_preprocessed)

# The optmal k is 4
# Apply KMeans clustering with the optimal number of clusters
k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_preprocessed)

"""Let's generate graphs for different customer segments based on their characteristics."""

# Create scatter plots for key feature relationships
plt.figure(figsize=(10, 7))

# Scatter plot of Age vs Income colored by Cluster
plt.subplot(2, 2, 1)
sns.scatterplot(data=df, x='Age', y='Income', hue='Cluster', palette='viridis', alpha=0.7)
plt.title('Age vs Income by Cluster')
plt.xlabel('Age')
plt.ylabel('Income')

# Scatter plot of Peak_minute_09 vs Offpeak_minute_09 colored by Cluster
plt.subplot(2, 2, 2)
sns.scatterplot(data=df, x='Peak_minute_09', y='Offpeak_minute_09', hue='Cluster', palette='viridis', alpha=0.7)
plt.title('Peak vs Offpeak Minutes (09) by Cluster')
plt.xlabel('Peak Minutes (09)')
plt.ylabel('Offpeak Minutes (09)')

plt.tight_layout()
plt.show()

key_features = ['Peak_minute_09', 'Weekend_minute_09', 'Offpeak_minute_09']

# Calculate average values for the selected features
average_values = cluster_summary[key_features]

# Create a bar plot for the average values
average_values.plot(kind='bar', figsize=(8, 4))
plt.title('Average Values of Key Features by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Average Value')
plt.xticks(rotation=0)
plt.legend(title='Features')
plt.grid(axis='y')
plt.show()

# Count plot for Gender distribution across clusters
plt.figure(figsize=(8, 4))
sns.countplot(data=df, x='Gender', hue='Cluster', palette='viridis')
plt.title('Gender Distribution by Cluster')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.legend(title='Cluster')
plt.grid(axis='y')
plt.show()

# Violin plot for Offpeak Minutes by Cluster
plt.figure(figsize=(8, 4))
sns.violinplot(data=df, x='Cluster', y='Offpeak_minute_09', palette='viridis')
plt.title('Distribution of Offpeak Minutes by Cluster')
plt.xlabel('Cluster')
plt.ylabel('Offpeak Minutes (09)')
plt.grid(axis='y')
plt.show()

"""# 3. Subtask: (explaination of clusters / conclusions)
Try to find an explanation of what characterizes each group and what characteristics caused each customer to be in the given cluster.
"""

# Analyze cluster characteristics
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cluster_summary = df.groupby('Cluster')[numerical_columns].mean()
# Display cluster summary
print(cluster_summary)

# Visualizing the mean values for each feature across clusters
cluster_summary.T.plot(marker='o', figsize=(10, 6))
plt.xticks(rotation=45)
plt.title('Feature Means by Cluster')
plt.xlabel('Features')
plt.ylabel('Mean Values')
plt.legend(title='Cluster')
plt.grid()
plt.show()

# Calculate the mean and standard deviation of the original Age feature
original_mean_age = df['Age'].mean()
original_std_age = df['Age'].std()

# Reversing the standardization for each cluster's mean age
for cluster in range(k):
    standardized_age = cluster_summary.loc[cluster, 'Age']  # Get standardized Age
    original_age = standardized_age * original_std_age + original_mean_age  # Inverse transform
    print(f"Cluster {cluster} - Original Mean Age: {original_age:.1f} years")

# Analyze cluster characteristics
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
cluster_summary = df.groupby('Cluster')[numerical_columns].mean()

# Reversing the standardization for each cluster's mean age
for cluster in range(k):
    standardized_age = cluster_summary.loc[cluster, 'Age']  # Get standardized Age
    original_age = standardized_age * original_std_age + original_mean_age  # Inverse transform
    print(f"Cluster {cluster} - Original Mean Age: {original_age:.1f} years")